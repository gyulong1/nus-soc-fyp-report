\chapter{Conclusion}

This chapter summarises our study's contributions and limitations. It also makes recommendations for further work that future projects could investigate.

\section{Contributions}
In this study, we have benchmarked $5$ QUBO solvers:
\begin{enumerate}
    \item D-Wave Quantum Annealer
    \item Neural Network Quantum States (NNQS)
    \item Quantum Approximate Optimization Algorithm (QAOA)
    \item GUROBI Optimizer
    \item Fixstars Amplify QUBO Solver
\end{enumerate}
We used three types of combinatorial optimisation problems: not-all-equal 3-satisfiability (NAE3SAT), max-cut, and Sherrington-Kirkpatrick model (SK model) problems as datasets to evaluate the solvers' performance.

Our results show that the D-Wave and NNQS solvers perform the best among the quantum-inspired solvers. The NNQS solver is generally better than the D-Wave solver except for the SK model dataset. The QAOA solver achieves generally poor performance across datasets and is not comparable due to the limitations on problem size. All three quantum-inspired solvers underperform the two classical solvers, with the simulated-annealing-based Fixstars solver achieving the best performance for all datasets.

When the runtimes of the D-Wave, GUROBI, and Fixstars solvers were matched, the D-Wave solver could outperform the GUROBI solver for specific ranges of problem sizes. This inspires further development of quantum annealers that can handle larger problems with dense connectivity.

We also investigated the NNQS solver with different architectures and training algorithms. The Restricted Boltzmann Machine (RBM) and the Multilayer Perceptron (MLP) were used as the underlying neural networks along with three different training algorithms---progressive, direct, and continuous. We found that the RBM with a continuous training scheme performs best across all datasets.

\section{Limitations and Future Work}
The primary constraint of our study came from the limitations of the QAOA solver, which was intended to be run on a gate-based quantum computer. Due to restricted availability of real quantum computers, we used a quantum simulator capable of handling only up to $30$ variables. Nevertheless, as the simulator does not model quantum noise and the results from the QAOA solver for small problems are not promising, QAOA on an real quantum computer would likely perform even worse for larger problems and is thus not too interesting to benchmark in the current NISQ era. Another limitation was that there were many parameters for each solver that we did not have the resources to optimise for, such as the annealing time for the D-Wave solver and various forms of the QAOA Ansatz. These might be more suitable for future projects that investigates one specific QUBO solver.

Future studies can explore more QUBO problem types and attempt to classify problems that are difficult for annealing-based solvers, such as quantum annealers and simulated annealers, but easier for other solvers like the QAOA solver. When gate-based quantum computers are readily available, the QAOA solver with larger $p$ values ($p > 1$) could also be benchmarked, which has been shown to perform better but requires more computational resources.

For further work with NNQS, future studies could investigate if more modern machine learning models, such as Graph Neural Networks or Attention-based Neural Networks, can be used as the underlying architecture for NNQS and whether they provide better performance. It would also be interesting to investigate why the continuous training scheme performs better than the progressive training scheme.

Future studies could also determine whether NNQS closely approximates the wave function of a D-Wave solver during the quantum annealing process. A quench of the D-Wave annealing process can help take a snapshot of the intermediate state, which can be compared to the intermediate sampling results from the NNQS solver. Extra information is included in \autoref{appendix:quenching}.