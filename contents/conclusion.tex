\chapter{Conclusion}

This chapter summarises our study's contributions and limitations. It also makes recommendations for further work that future projects could investigate.

\section{Contributions}
In this study, we have benchmarked $5$ QUBO solvers:
\begin{enumerate}
    \item D-Wave Quantum Annealing
    \item Neural Network Quantum States (NNQS)
    \item Quantum Approximate Optimization Algorithm (QAOA)
    \item GUROBI Optimizer
    \item Fixstars Amplify QUBO Solver
\end{enumerate}
$3$ datasets were used which comprised of $3$ types of combinatorial optimization problems:
\begin{enumerate}
    \item Not-all-equal 3-satisfiability (NAE3SAT)
    \item Max-cut
    \item Sherrington-Kirkpatrick model (SK model)
\end{enumerate}
Our results show that the D-wave and NNQS solvers perform best among the quantum-inspired solvers. The NNQS solver is generally better than the D-wave solver except for the SK model dataset. The QAOA solver achieves generally poor performance across datasets and is not comparable due to the lack of large-scale gate-based quantum computers. All three quantum-inspired solvers underperform the two classical solvers, with the simulated-annealing-based Fixstar Amplify QOBO solver achieving the best performance out of all solvers for all datasets.

We also investigated the NNQS solver with different architectures and training algorithms. The Restricted Boltzmann Machine (RBM) and the Multilayer Perceptron (MLP) were used as the underlying neural networks along with $3$ different training algorithms---progressive, direct, and continuous. We found that using the RBM with a continuous training scheme gives the best performance across all datasets.

The primary constraint of our study came from the limitations of the QAOA solver, which was intended to be run on a gate-based quantum computer. Due to restricted availability, we could only use a simulator capable of handling only up to $30$ variables. However, as the QAOA simulator does not model quantum noise and the results from the QAOA solver for small problems are not promising, QAOA on an actual quantum device would likely perform even worse for larger problems and is thus not too interesting to benchmark in the current NISQ era. The other significant limitation was that there were many parameters for each solver that we did not have the resources to optimise for, such as the annealing time for the D-Wave solver and the training scheme for the NNQS solver. These might be more suitable for a future project that optimises a specific quantum-inspired QUBO solver.

\section{Future work}
Future studies can explore more QUBO problem types and attempt to classify problems that are difficult for annealing-based solvers such as QA and SA but easier for other solvers such as QAOA. When gate-based quantum computers are readily available, the QAOA solver with larger $p$ values ($p > 1$) could be benchmarked, likely leading to better performance.

For further work with NNQS, future studies could investigate if more modern machine learning models, such as Graph Neural Networks or Attention-based Neural Networks, can be used as the underlying architecture for NNQS and whether they provide better performance.

Future work could also investigate whether NNQS closely approximates the wave function of a D-wave solver in the quantum annealing process. This could be done by conducting a quench of the D-wave annealing process to take a snapshot of the intermediate state. Some initial work is detailed in \autoref{appendix:quenching} but has not been included in the main report as the results are not substantial.